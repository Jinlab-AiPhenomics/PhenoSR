name: train_phenosr
model_type: PhenoSRModel
scale: 4
num_gpu: 1  # auto: can infer from your visible devices automatically.
manual_seed: 0
stage: train

gt_usm: True 

resize_prob: [ 0.2, 0.7, 0.1 ]  # up, down, keep
resize_range: [ 0.15, 1.5 ]
gaussian_noise_prob: 0.5
noise_range: [ 1, 30 ]
poisson_scale_range: [ 0.05, 3 ]
gray_noise_prob: 0.4
jpeg_range: [ 30, 95 ]

second_blur_prob: 0.8
resize_prob2: [ 0.3, 0.4, 0.3 ]
resize_range2: [ 0.3, 1.2 ]
gaussian_noise_prob2: 0.5
noise_range2: [ 1, 25 ]
poisson_scale_range2: [ 0.05, 2.5 ]
gray_noise_prob2: 0.4
jpeg_range2: [ 30, 95 ]

gt_size: 256
queue_size: 192

datasets:
  train:
    name: PhenoSR
    type: PhenoSRDataset
    # the path of sr dataset
    dataroot_gt: ~
    # the path of the meta info file
    meta_info: ~
    io_backend:
      type: disk

    blur_kernel_size: 21
    kernel_list: [ 'iso', 'aniso', 'generalized_iso', 'generalized_aniso', 'plateau_iso', 'plateau_aniso' ]
    kernel_prob: [ 0.45, 0.25, 0.12, 0.03, 0.12, 0.03 ]
    sinc_prob: 0.1
    blur_sigma: [ 0.2, 3 ]
    betag_range: [ 0.5, 4 ]
    betap_range: [ 1, 2 ]

    blur_kernel_size2: 21
    kernel_list2: [ 'iso', 'aniso', 'generalized_iso', 'generalized_aniso', 'plateau_iso', 'plateau_aniso' ]
    kernel_prob2: [ 0.45, 0.25, 0.12, 0.03, 0.12, 0.03 ]
    sinc_prob2: 0.1
    blur_sigma2: [ 0.2, 1.5 ]
    betag_range2: [ 0.5, 4 ]
    betap_range2: [ 1, 2 ]

    final_sinc_prob: 0.8

    gt_size: 256
    use_hflip: True
    use_rot: False

    use_shuffle: true
    num_worker_per_gpu: 5
    batch_size_per_gpu: 2
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

network_g:
  type: PhenoSR
  upscale: 4
  in_chans: 3
  img_size: 64
  window_size: 8
  img_range: 1.
  coarse_depths: [ 6, 6]
  refine_depths: [ 6, 6, 6, 6, 6, 6 ]
  embed_dim: 180
  coarse_num_heads: [ 6, 6 ]
  refine_num_heads: [ 6, 6, 6, 6, 6, 6 ]
  mlp_ratio: 2
  seg_dim: 32
  resi_connection: '1conv'
  # the number of classes in the segmentation model
  num_classes: ~
  # the path of the modified HRNet model
  seg_model_path: ~

path:
  pretrain_network_g: ~
  param_key_g: params_ema
  strict_load_g: false
  resume_state: ~
  # Modify it to your storage path
  root_path: ~

train:
  freeze: true
  ema_decay: 0.999
  optim_g:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [ 0.9, 0.99 ]

  scheduler:
    type: MultiStepLR
    milestones: [ 1000000 ]
    gamma: 0.5

  total_iter: 400000
  warmup_iter: -1  # no warm up

  pixel_opt:
    type: L1Loss
    loss_weight: 1.0
    reduction: mean


# logging settings
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29500
